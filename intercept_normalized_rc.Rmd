---
title: "Changing from robust to Bayesian regression with a Gaussian
error model"
author: "Maarten Slagter"
output: html_document
fig_width: 8.7/2.54
fig_height: 8.7/2.54
fig_align: center
---

# Evidence/BF explanation

In this document we develop a Bayesian version of the PS vs. NAYR
regression, which allows for normalization of the rc by the intercept.
The advantage of this normalization step is that it allows for direct
comparison between sub-analyses, which are subject to varying levels
of neo-antigen detection sensitivity and precision.

```{r, warning=FALSE}
source('~/antigenic_space/bin/init.R')
knitr::opts_chunk$set(cache = T, include = T)
source(file.path('~/antigenic_space', 'maarten-analyses',
    'immune_editing', 'continuous_IE_detection_init.R'))

reg_method <- 'bayesian_biased'

if (F) {
  pe = "Pan*'-'*cancer"
  ## Get all analyses...
  setting_dtf <- prep_pan_IE_heatmap(
    tumor_type = pe,
    include_non_ds_tallies = T,
    ncores = ncores,
    fill_var = 'estimate',
    reg_method = reg_method,
    z_normalize = F,
    pick_allele_method = 'est_error',
    filter_intercept = 'none',
    cell_size_determinant = 'log2_est_error',
    filter_error = 0,
    make_filtering_diagnostic_plots = T,
    do_test_grobs = F
  )
} else {
  setting_dtf <- compile_all_coef_overview(
    redo = F,
    ncores = 24,
    reg_method = reg_method,
    include_non_ds_tallies = T,
    z_normalize = F
  )
  f_setting_dtf <- format_coef_overview(
    dtf = setting_dtf,
    # plot_fishtails = 'project',
    plot_fishtails = NULL,
    filter_intercept = F,
    intercept_filter_error = 4,
    # rc_filter_error = 4,
    delta_filter_error = NULL
  )
}
# plot_bayesian_error_vs_effect(setting_dtf, grouping = 'project')
```

```{r, warning=FALSE}
# devtools::load_all(file.path('~/antigenic_space', 'libs', 'fasanalysis'))
source(file.path('~/antigenic_space', 'maarten-analyses', 'immune_editing',
                 'continuous_IE_detection_init.R'))

idx = 2
## Select an especially interesting one...
pick <- setting_dtf[star == 'L' & maartenutils::eps(estimate, -.10)][1]

## Extract arguments to be passed to modelling functions
args <- pick %>%
  pan_IE_res_to_call_args() %>%
  modifyList(
    list(
      'z_normalize' = F, redo = F,
      tumor_type = pick$project_extended
    )
  )

## Prepare and compile data
# formals(prep_cont_IE_analyses)
prep <- call_func(prep_cont_IE_analyses, args)
```


```{r, warning=FALSE}
## Run modelling function in order to anecdotally compare models
r_z_lm <- call_func(test_continuous_IE,
  modifyList(args, list('reg_method' = 'rlm_one_group', 'z_normalize' = T))) %>%
  { .$stats[[pe]][[1]]$lm }
  fit_bayesian_regression

r_lm <- call_func(test_continuous_IE,
  modifyList(args, list('redo' = T, 'reg_method' = 'rlm_one_group',
      'z_normalize' = F))) %>%
  { .$stats[[pe]][[1]]$lm }
```

```{r, warning=FALSE}
source(file.path('~/antigenic_space', 'maarten-analyses', 'immune_editing',
                 'continuous_IE_detection_init.R'))

## Run Bayesian modelling functions
b_g_lm <- fit_bayesian_regression(prep$dtf, prior = 'biased',
  return_val = 'lm')

b_t_lm <- fit_bayesian_regression(prep$dtf, prior = 'biased',
  family = 'student', return_val = 'lm')

# stored_lm <- call_func(test_continuous_IE, args)$stats[[pe]][[1]]
# stored_lm[c('intercept_estimate', 'ol_estimate')]
```

```{r, fig.width = 8.7/2.54, fig.height = 8.7/2.54, warning=FALSE}
source(file.path('~/antigenic_space', 'maarten-analyses', 'immune_editing',
                 'continuous_IE_detection_init.R'))
plot_id_base <- paste(args[analysis_grp_vars], collapse = '-')

p <- plot_PS_vs_yr(dtf = prep$dtf, reg_method = 'rlm_one_group', lm = r_lm)
# print(p)
# plot_id <- paste0(plot_id_base, '-rlm')
# ggsave(file.path(img_loc, glue('example_scatter-{plot_id}.png')), p)

p <- plot_PS_vs_yr(dtf = prep$dtf, reg_method = 'bayesian_biased', lm = b_g_lm)
print(p)
# plot_id <- paste0(plot_id_base, '-bayesian-biased')
# ggsave(file.path(img_loc, glue('example_scatter-{plot_id}.png')), p)

p <- plot_PS_vs_yr(dtf = prep$dtf, reg_method = 'bayesian_biased', lm = b_t_lm)
print(p)
```

The robust t-model does better for these particular data, but finds somewhat
different parameters than the robust (`rlm`) function. This is likely due to the
scaling that happens to the data and/or the different parameterization of the
t-distribution.

```{r, warning=FALSE}
loo(b_g_lm, b_t_lm)
```

# Negative control

When the dependent variable (NAYR) is permuted, such that PS
and NAYR are no longer associated, the model finds a slope of 0 on average.

```{r, warning=FALSE}
source(file.path('~/antigenic_space', 'maarten-analyses',
    'immune_editing', 'continuous_IE_detection_init.R'))

N_repeats = 1000
plan(multicore, workers = 32)

redo = T
redo = F

args <- list(
  overlap_var = "mean_score", patient_inclusion_crit = "TR",
  LOH_HLA = "strict_LOHHLA", analysis_name = "twoD_sens_analysis",
  focus_allele = "B2705", analysis_idx = 17L, include_call = TRUE,
  redo = FALSE, z_normalize = FALSE, reg_method = "bayesian_biased",
  project_extended = "Pan*'-'*cancer"
)
args <-
  list(
    overlap_var = 'mean_score', 
    patient_inclusion_crit = 'strict_TR',
    LOH_HLA = 'LOHHLA', 
    analysis_name = 'marty_param_titration',
    focus_allele = 'A1101', 
    analysis_idx = 31L, 
    include_call = TRUE,
    redo = FALSE, z_normalize = FALSE, 
    reg_method = 'bayesian_biased',
    tumor_type = "Pan*'-'*cancer", 
    project_extended = "Pan*'-'*cancer"
  )
print(pick)

prep <- call_func(prep_cont_IE_analyses, args)

o_fn <- file.path(rds_dir, glue::glue('bayes_negative_control.rds'))
if (!file.exists(o_fn) || redo) {
  neg_c <- bayes_negative_control(prep)
  saveRDS(neg_c, o_fn)
} else {
  neg_c <- readRDS(o_fn)
}
setDT(neg_c)[, 'original' := i == 1]
```

```{r, warning=FALSE}
neg_c %>%
  .[sampling_convergence == T] %>%
  .[est_error <= 1] %>%
  ggplot(aes(x = estimate, colour = original)) + geom_boxplot()

neg_c %>%
  .[sampling_convergence == T] %>%
  .[est_error <= 1] %>%
  .[, mean(.SD[1, estimate] >= .SD[-1, estimate])]
```

```{r, figure.height = 8.7/2.54, figure.width = 8.7/2.54, warning=FALSE}
mean(neg_c[, 'ol_rhat'] > 1.01)
mean(neg_c[, sampling_convergence])

neg_c %>%
  dplyr::filter(ol_rhat < 1.01 & intercept_rhat < 1.01) %>%
  dplyr::summarize(across(c('intercept_estimate', 'ol_estimate'),
      list('min' = min, 'mean' = mean, 'max' = max))) %>%
  print(width = 120)

# hist(neg_c$est_error, breaks = 1000)


p <- neg_c %>%
  .[sampling_convergence == T] %>%
  .[est_error <= 1] %>%
  # dplyr::filter(ol_rhat < 1.01 & intercept_rhat < 1.01) %>%
  dplyr::filter(ol_rhat < 1.01 & intercept_rhat < 1.01) %>%
  dplyr::select('D' = estimate, 'intercept' = intercept_estimate,
    'slope' = ol_estimate) %>%
  GGally::ggpairs(aes(colour = 'original'),
    diag = list(continuous = 'barDiag'),
    upper = list(continuous = 'density', combo = 'facethist')
  ) + theme_fas()

print_plot(p, fn = file.path(img_loc, 'bayes_reg_neg_control_pairs_plot.png'))
```

# Systematic negative control, on multiple subanalyses

Caution: sub-analyses are picked from a set that converged and so is (slightly?)
more likely to adhere to a linear association

```{r, eval = F, warning=FALSE}
source(file.path('~/antigenic_space', 'maarten-analyses',
    'immune_editing', 'continuous_IE_detection_init.R'))

N_repeats = 1000
N_repeats = 10000
N_repeats = 100
N_subanalyses = 100
N_subanalyses = 25
N_subanalyses = 50
N_subanalyses = 1000
set.seed(2)
[, 'log2_dev_from_perm' :=
  log2(evid_ratio / (1 + evid_ratio)) - log2(1 - perm_p)]

saveRDS(neg_c, file.path(rds_dir, 'systematic_neg_c.rds'))
neg_c[order(log2_dev_from_perm)]
```

```{r, eval = F, warning=FALSE}
p <- ggplot(neg_c, aes(x = -log2(evid_ratio), y = perm_p,
    colour = log10(n_patients))) +
  geom_point() +
  geom_smooth(method = 'gam') +
  ylab('Permutation p-value\n(fraction of permutations with lower\nor equal mean D)') +
  xlab('-log2 evidence ratio')
print_plot(p, 
  fn = file.path(img_loc, 'bayes_reg_systematic_neg_control.png'))
```

```{r, eval = F, warning=FALSE}
p <- neg_c %>%
  dplyr::filter(sampling_convergence == T) %>%
  dplyr::arrange(est_error) %>%
  dplyr::mutate('acceptable_error' = -log2(est_error) > 2) %>%
  dplyr::mutate(acceptable_error = factor(acceptable_error, levels = c(T, F),
      labels = c('Acceptable error', 'Unacceptable error'))) %>%
  ggplot(aes(x = evid_ratio / (1 + evid_ratio), y = 1 - perm_p,
      colour = -log2(est_error))) +
  geom_point() +
  geom_smooth(method = 'lm', formula = ~1 + x) +
  scale_colour_gradient2(name = '-log2(estimation error)', mid = 'grey80') +
  ylab('Permutation p-value\n(fraction of permutations with\nmean D >= original D)') +
  xlab('ER / (1 + ER)') +
  facet_grid(~acceptable_error) +
  theme_fas(panel.spacing = unit(.8, 'lines'))

print_plot(p, fn = file.path(img_loc, 'bayes_reg_systematic_neg_control.png'))
```

# 2nd negative control: titrate slope to see effect on estimation error

Do we enrich for stronger effects by filtering on estimation error?

```{r, warning=FALSE}
source(file.path('~/antigenic_space', 'maarten-analyses', 
    'immune_editing', 'continuous_IE_detection_init.R'))

pick <- setting_dtf[sample(1:nrow(setting_dtf), 1)]
pick <- setting_dtf[
  n_patients > 100 & 
  maartenutils::eps(yr_fractional_change, 0, 1e-2)][1]
  # maartenutils::eps(estimate, 0, 1e-2)][1]

## Extract arguments to be passed to modelling functions
args <- pick %>%
  pan_IE_res_to_call_args() %>%
  modifyList(
    list(
      'z_normalize' = F, redo = F,
      tumor_type = pick$project_extended
    )
  )

## Prepare and compile data
# formals(prep_cont_IE_analyses)
prep <- call_func(prep_cont_IE_analyses, args)

## Extract arguments to be passed to modelling functions

# args <- pick %>%
#   pan_IE_res_to_call_args() %>%
#   modifyList(list(
#       'z_normalize' = F, redo = F,
#       reg_method = reg_method)) %>%
#   # { append(., list('project_extended' =
#   #     unname(tumor_types_inv[as.character(.$tumor_type)]))) } %>%
#   { append(., list('tumor_type' =
#       unname(tumor_types_inv[as.character(.$project_extended)]))) } %>%
#   # append(list('project_extended' = pick$project_extended)) %>%
#   # append(list(tumor_type = NULL))
#   { .[['tumor_type']] <- NULL; .}
```

```{r, warning=FALSE}
source(file.path('~/antigenic_space', 'maarten-analyses',
    'immune_editing', 'continuous_IE_detection_init.R'))
source(file.path(ma_dir, 'immune_editing', 'load_brms_models.R'))
# fit <- load_intercept_matched_brm(prep$dtf, intercept_resolution = .01)
rlm_fit <- rlm_fit_model(prep$dtf)
```

```{r, warning=FALSE}
source(file.path('~/antigenic_space', 'maarten-analyses',
    'immune_editing', 'continuous_IE_detection_init.R'))

lower_bound = T
lower_bound = F
beta_x = F
beta_x = T
scale_intercept = -1
scale_intercept = 5
scale_intercept = 1
intercept_scale = 0.05
intercept_scale = 0.005
intercept_scale = 0.1

library(furrr)
plan(multicore, workers = 32)
plan(transparent)

#model_name = intercept_matched'
model_name = 'bayesian_biased'
model_name = 'rlm'

rounded_intercept <- round(10 * pick$intercept) / 10
rounded_intercept <- scale_intercept * pick$intercept

all_tests <- 
  expand_grid(
    rc = seq(
      -3 * abs(rounded_intercept), 3 * abs(rounded_intercept),
      by = abs(rounded_intercept)
    ),
    noise_bound = seq(0, .3, .1),
    rep = 1:3
  ) %>%
  pipe_row_print %>%
  future_pmap_dfr(
    sim_and_test,
    prep = prep,
    beta_x = beta_x,
    orig_fit = model_name,
    intercept_scale = intercept_scale,
    scale_intercept = scale_intercept
  )

x_scale <-
  scale_x_continuous(
    name = expression(beta[1]~'in multiples of intercept'),
    breaks = unique(all_tests$rc),
    labels = unique(all_tests$rc / abs(rounded_intercept))
  )

p0 <- ggplot(all_tests,
  aes(y = intercept_estimate, x = rc,
    colour = as.factor(noise_bound))) +
  geom_hline(yintercept = intercept_scale, colour = 'grey50') +
  geom_point(size = .5) +
  ylab(expression(hat(beta[0]))) +
  scale_colour_discrete(name = 'Noise level') +
  x_scale +
  theme_fas()

p0_e <- ggplot(all_tests,
  aes(y = intercept_est_error, x = rc,
    colour = as.factor(noise_bound))) +
  geom_point(size = .5) +
  ylab(expression('SE'~hat(beta[0]))) +
  scale_colour_discrete(name = 'Noise level') +
  x_scale +
  theme_fas()

p_rc <- ggplot(all_tests,
  aes(y = ol_estimate, x = rc, colour = as.factor(noise_bound))) +
  geom_abline(intercept = 0, slope = intercept_scale, 
    colour = 'grey50') +
  geom_point(size = .5) +
  ylab(expression(hat(beta[1]))) +
  scale_colour_discrete(name = 'Noise level') +
  x_scale +
  theme_fas()

p_rc_e <- ggplot(all_tests,
  aes(y = rc_error, x = rc,
      colour = as.factor(noise_bound))) +
  geom_point(size = .5) +
  ylab(expression('SE'~hat(beta[1]))) +
  scale_colour_discrete(name = 'Noise level') +
  x_scale +
  theme_fas()

p_delta <- ggplot(all_tests,
  aes(y = estimate, x = rc,
      colour = as.factor(noise_bound))) +
  geom_abline(intercept = 0, slope = 1,
    colour = 'grey50') +
  geom_point(size = .5) +
  scale_colour_discrete(name = 'Noise level') +
  ylab(expression(hat(Delta))) +
  x_scale +
  theme_fas()

p_delta_e <- ggplot(all_tests,
  aes(y = est_error, x = rc,
      colour = as.factor(noise_bound))) +
  geom_point(size = .5) +
  scale_colour_discrete(name = 'Noise level') +
  ylab(expression('SE'~hat(Delta))) +
  # geom_smooth()
  x_scale +
  theme_fas()

library(patchwork)
print_plot((p0 + p_rc + p_delta) / (p0_e + p_rc_e + p_delta_e) +
  plot_layout(guides = 'collect'),
  fn = file.path(img_loc,
    glue('negative_control_noise_vs_est_error\\
    {make_flag(model_name)}\\
    {make_flag(beta_x)}\\
    {make_flag(intercept_scale)}\\
    {make_flag(rounded_intercept)}\\
    {make_flag(lower_bound)}.png')
  ), h = 10, w = 17.4)

```

```{r, warning=FALSE}
source(file.path('~/antigenic_space', 'maarten-analyses', 
    'immune_editing', 'continuous_IE_detection_init.R'))

slope = .5
slope = -pick$intercept_estimate

plots <- future_map(c(-slope, slope), function(s) {
  p1 <- sim_and_plot(
    prep = prep,
    rc = s,
    noise_bound = .3
  )
}) %>% unlist(recursive = F)

patchwork::wrap_plots(plots, guides = 'collect') %>%
  print_plot(
    fn = file.path(img_loc, 
      'negative_control_noise_vs_est_error_examples.png'),
    h = 10,
    w = 10
  )
```

# RLM analog

RLM based try to do the same, but much more computationally efficient
and quite possibly more statistically robust. 

```{r, warning=FALSE}
source(file.path('~/antigenic_space', 'maarten-analyses',
    'immune_editing', 'continuous_IE_detection_init.R'))

pick <- setting_dtf[maartenutils::eps(estimate, -.10)][2]

## Extract arguments to be passed to modelling functions
args <- pick %>%
  pan_IE_res_to_call_args() %>%
  modifyList(
    list(
      'z_normalize' = F, redo = F,
      tumor_type = pick$project_extended
    )
  )

## Prepare and compile data
# formals(prep_cont_IE_analyses)
prep <- call_func(prep_cont_IE_analyses, args)
```

Data scaling doesn't affect rlm model convergence or p-values at all.
Of course, it does affect the 'scale' (i.e. MAD of residuals).

```{r, warning=FALSE}
source(file.path('~/antigenic_space', 'maarten-analyses',
    'immune_editing', 'continuous_IE_detection_init.R'))

dtf <- prep$dtf %>%
  apply_intercept_scale(
    find_intercept_method = 'rlm',
    intercept_scale = 1
  ) %>%
  { . }

mod <- fit_rlm_model(dtf)

1 / mod$intercept * mod$scale

plot_PS_vs_yr(
  dtf = dtf, 
  lm = mod$lm, 
  colour_vars = NULL, 
  shape_var = NULL
)
```


# Compare robust and Bayesian model coefficients for randomly selected sub-analyses

We take a particular subanalysis (i.e. combination of pipeline and
immunoediting analysis settings) and perform the regression both with
the robust regression method , Bayesian method with unbiased prior
(i.e. symmetric prior for intercept) and Byesian method with biased
priors (exponential prior for intercept).

```{r, results='asis', warning=FALSE}
source(file.path('~/antigenic_space', 'maarten-analyses',
    'immune_editing', 'continuous_IE_detection_init.R'))

sample_subs <- dtf[order(yr_fractional_change)]

## Big difference between priors
idx = 67

pe = "Pan*'-'*cancer"

# knitr::kable(compare_methods(idx), n = 1000, width = 1000)
knitr::kable(compare_methods(pick = pick), n = 1000, width = 1000)
# knitr::kable(compare_methods(2765), n = 1000, width = 1000)
# knitr::kable(compare_methods(7724), n = 1000, width = 1000)
# knitr::kable(compare_methods(10004), n = 1000, width = 1000)
# knitr::kable(compare_methods(2495), n = 1000, width = 1000)
# knitr::kable(compare_methods(7469), n = 1000, width = 1000)
# knitr::kable(compare_methods(7469), n = 1000, width = 1000)
```

```{r, warning=FALSE}
N_repeats = 100
N_repeats = 50
N_repeats = 30
N_repeats = 49
N_repeats = 1000
redo = F
plan(multicore, workers = 32)
plan(multicore, transparent)

o_fn <- file.path(rds_dir, 
  glue::glue('bayes_vs_rlm_systematic_comp.rds'))
# file.remove(o_fn)
# back_up(o_fn)
if (!file.exists(o_fn) || redo) {
  set.seed(2)
  bayes_vs_rlm_systematic_comp <-
    sample(1:nrow(sample_subs), N_repeats, replace = F) %>%
    furrr::future_map_dfr(function(i) {
      res <- tryCatch(compare_methods(i),
        error = function(e) { print(i); print(e); NULL })
      # print(i)
      # print(res)
    }, .id = 'i')
  saveRDS(bayes_vs_rlm_systematic_comp, o_fn)
} else {
  bayes_vs_rlm_systematic_comp <- readRDS(o_fn)
}
```

```{r, warning=FALSE}
bayes_vs_rlm_systematic_comp %>%
  dplyr::filter(ol_rhat < 1.01 & intercept_rhat < 1.01) %>%
  dplyr::filter(sampling_convergence == TRUE) %>%
  dplyr::summarize(across(
      c('estimate', 'intercept_estimate', 'ol_estimate'),
      list('min' = min, 'mean' = mean, 'max' = max))) %>%
  print(width = 120)

p <- bayes_vs_rlm_systematic_comp %>%
  dplyr::filter(ol_rhat < 1.01 & intercept_rhat < 1.01) %>%
  dplyr::filter(sampling_convergence == TRUE) %>%
  dplyr::filter(used_prior == 'biased') %>%
  dplyr::select(
    'intercept' = intercept_estimate,
    'slope' = ol_estimate,
    'diff_nayr' = estimate,
    'rlm_intercept' = rlm_intercept,
    'rlm_slope' = rlm_rc,
    'rlm_diff_nayr' = rlm_rc / rlm_intercept
  ) %>%
  GGally::ggpairs(diag = list(continuous = 'barDiag')) +
  theme_fas()

print(p)
```

# Visualize individual sub-analyses

```{r, warning=FALSE}
stopifnot(merged[, all(n_patients.x == n_patients.y)])
# merged[, summary(rc), by = sampling_convergence]
res <- merged[, .N, by = sampling_convergence] %>%
  add_frac() %>%
  { . }
```

```{r, warning=FALSE}
source(file.path('~/antigenic_space', 'maarten-analyses', 'immune_editing',
                 'continuous_IE_detection_init.R'))

## Editing but not significant
pick <- bayesian_g_dtf[
  maartenutils::eps(est_error, 1) &
  sampling_convergence == T & evid_ratio > 1 & estimate < 1][1]

inspect_coefs(pick = pick)
```

```{r, warning=FALSE}
source(file.path('~/antigenic_space', 'maarten-analyses', 
    'immune_editing', 'continuous_IE_detection_init.R'))
## 40% Editing and significant
pick <- bayesian_g_dtf[
  maartenutils::eps(estimate, -.4) &
  # log2(est_error) <= -4 &
  sampling_convergence == T & evid_ratio > 1]

## 40% enrichment and significant
pick <- bayesian_g_dtf[
  maartenutils::eps(estimate, .4) &
  # log2(est_error) <= -4 &
  sampling_convergence == T & evid_ratio < 1]

## 40% Editing and insignificant
pick <- bayesian_g_dtf[
  maartenutils::eps(estimate, -.4) &
  maartenutils::eps(evid_ratio, 2) &
  sampling_convergence == T]
# pick$est_error

## 40% enrichment and insignificant
# pick <- bayesian_g_dtf[
#   maartenutils::eps(estimate, .4) &
#   maartenutils::eps(evid_ratio, 2) &
#   sampling_convergence == T]

print(pick[, .N])
print(pick[1])
```

```{r, warning=FALSE}
inspect_coefs(pick = pick[1])
```

```{r, warning=FALSE}
# Test stability of results as function of MCMC iterations

args <- pick[1] %>%
  pan_IE_res_to_call_args() %>%
  {
    modifyList(., list(redo = F, tumor_type = pick$project_extended,
        'z_normalize' = F))
  }

prep <- call_func(prep_cont_IE_analyses, args)

res <- fit_bayesian_regression(prep$dtf, model_name = 'biased_fit',
  N_iter = 50000)
```

```{r, warning=FALSE}
# bayesian_g_dtf[sampling_convergence == F, .N]
# bayesian_g_dtf[sampling_convergence == T, .N]
# bayesian_g_dtf[, all(yr_fractional_change == estimate)]
# bayesian_g_dtf[, yr_fractional_change == estimate)]
# bayesian_g_dtf[, summary(evid_ratio)]
# bayesian_g_dtf[, summary(log10(evid_ratio))]
# bayesian_g_dtf[, yr_fractional_change]
```

```{r, warning=FALSE}
print_plot({
  ggplot(bayesian_g_dtf[sampling_convergence == T],
    aes(y = yr_fractional_change, x = log10(evid_ratio))) +
    geom_hex(bins = 300) +
    scale_fill_viridis_c() +
    scale_y_continuous(limits = c(-1, 1))
  }, w = 8.7, h = 8.7,
  fn = file.path(img_loc, 'bayesian_evidence_vs_D.png'))
```

```{r, warning=FALSE}
source(file.path('~/antigenic_space', 'maarten-analyses', 'immune_editing',
                 'continuous_IE_detection_init.R'))

print_overview_stats(bayesian_g_dtf,
  stage_id = 'filtered_for_MCMC_convergence',
  plot_fishtails = 'project')
```


```{r, warning=FALSE}
print_plot({
  bayesian_g_dtf[sampling_convergence == T] %>%
    select(rc = ol_estimate, log10_evid_ratio = evid_ratio) %>%
    dplyr::mutate(log10_evid_ratio = log10(log10_evid_ratio)) %>%
    GGally::ggpairs(
      lower = list(continuous = lower_hex),
      diag = list(continuous = diag_hist)
    ) + theme_classic()
  }, w = 8, h = 8,
  fn = file.path(img_loc, 'ggpairs_evid_ratio_vs_rc.png'))
```

```{r, warning=FALSE}
print_plot({
  bayesian_g_dtf[sampling_convergence == T] %>%
    select(rc = ol_estimate, log10_evid_ratio = evid_ratio) %>%
    dplyr::mutate(log10_evid_ratio = log10(log10_evid_ratio)) %>%
    GGally::ggpairs(
      lower = list(continuous = lower_hex),
      diag = list(continuous = diag_hist)
    ) + theme_classic()
  }, w = 8, h = 8,
  fn = file.path(img_loc, 'ggpairs_evid_ratio_vs_rc.png'))
```

#Visualize gamma dist

```{r, warning=FALSE}
## Gamma function experimentation
## The mean is at alpha/beta = 2/2 = 1
## The mode is at (l*alpha-1)/(l * beta) => 1 as l => Inf
f <- function(x) dgamma(x, shape = 2, rate = 2)
curve(f, 0, 5, plot = F)
is <- 1:5
for (i in is) {
  f <- function(x) dgamma(x, shape = i*2, rate = i*2)
  curve(f, 0, 5, add = T, col = i)
  abline(v = (i*2-1)/(2*i), col = i, lty = 3)
}
legend('topright', legend=is, fill = is)
```

